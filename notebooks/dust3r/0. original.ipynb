{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a33080c-921f-4b5f-8373-477481fd69f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "dust3r_path = str(Path(os.path.join(os.getcwd())).parent / \"dust3r\" / \"dust3r\")\n",
    "sys.path.append(dust3r_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06e37a28-f035-4408-8fcf-376c533d88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../../dust3r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a60740-13a5-4577-b676-8ab1f76a184b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinlu/miniconda3/envs/annotated-dust3r/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dust3r.cloud_opt import GlobalAlignerMode, global_aligner\n",
    "from dust3r.image_pairs import make_pairs\n",
    "from dust3r.inference import inference\n",
    "from dust3r.model import AsymmetricCroCo3DStereo\n",
    "from dust3r.utils.image import load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53139cd3-38b4-4aa2-80aa-3518bc62734a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "batch_size = 1\n",
    "schedule = \"cosine\"\n",
    "lr = 0.01\n",
    "niter = 300\n",
    "\n",
    "model_name = \"naver/DUSt3R_ViTLarge_BaseDecoder_512_dpt\"\n",
    "# you can put the path to a local checkpoint in model_name if needed\n",
    "model = AsymmetricCroCo3DStereo.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c888c-cca1-41aa-8321-e713be37e52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Loading a list of 3 images\n",
      " - adding ../assets/Chateau1.png with resolution 224x224 --> 512x384\n",
      " - adding ../assets/Chateau2.png with resolution 224x224 --> 512x384\n",
      " - adding ../assets/Chateau2.png with resolution 224x224 --> 512x384\n",
      " (Found 3 images)\n"
     ]
    }
   ],
   "source": [
    "# load_images can take a list of images or a directory\n",
    "images = load_images([\"../assets/Chateau1.png\", \"../assets/Chateau2.png\"], size=512)\n",
    "\n",
    "assert list(images[0].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"], images[0].keys()\n",
    "assert images[0][\"img\"].shape == torch.Size([1, 3, 384, 512])\n",
    "assert np.all(images[0][\"true_shape\"]) == np.all(np.array([[384, 512]]))\n",
    "assert images[0][\"idx\"] == 0\n",
    "assert images[0][\"instance\"] == \"0\"\n",
    "\n",
    "assert list(images[1].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"], images[1].keys()\n",
    "assert images[1][\"img\"].shape == torch.Size([1, 3, 384, 512])\n",
    "assert np.all(images[1][\"true_shape\"]) == np.all(np.array([[384, 512]]))\n",
    "assert images[1][\"idx\"] == 1\n",
    "assert images[1][\"instance\"] == \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd7abb19-4009-4e68-99fb-41e5a3f57ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load_images can take a list of images or a directory\n",
    "# images = load_images([\"../assets/Chateau1.png\", \"../assets/Chateau2.png\", \"../assets/Chateau2.png\"], size=512)\n",
    "\n",
    "# assert list(images[0].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"], images[\n",
    "#     0\n",
    "# ].keys()\n",
    "# assert images[0][\"img\"].shape == torch.Size([1, 3, 384, 512])\n",
    "# assert np.all(images[0][\"true_shape\"]) == np.all(np.array([[384, 512]]))\n",
    "# assert images[0][\"idx\"] == 0\n",
    "# assert images[0][\"instance\"] == \"0\"\n",
    "\n",
    "# assert list(images[1].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"], images[\n",
    "#     1\n",
    "# ].keys()\n",
    "# assert images[1][\"img\"].shape == torch.Size([1, 3, 384, 512])\n",
    "# assert np.all(images[1][\"true_shape\"]) == np.all(np.array([[384, 512]]))\n",
    "# assert images[1][\"idx\"] == 1\n",
    "# assert images[1][\"instance\"] == \"1\"\n",
    "\n",
    "# assert list(images[2].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"], images[\n",
    "#     2\n",
    "# ].keys()\n",
    "# assert images[2][\"img\"].shape == torch.Size([1, 3, 384, 512])\n",
    "# assert np.all(images[2][\"true_shape\"]) == np.all(np.array([[384, 512]]))\n",
    "# assert images[2][\"idx\"] == 2\n",
    "# assert images[2][\"instance\"] == \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "021bdcee-bb7e-4bdf-8274-c56b1edda925",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = make_pairs(images, scene_graph=\"complete\", prefilter=None, symmetrize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c417e605-edfd-4d9f-83a6-8def8fc6916e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m pairs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m (images[\u001b[38;5;241m1\u001b[39m], images[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[43mpairs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "assert pairs[0] == (images[1], images[0])\n",
    "assert pairs[1] == (images[0], images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa04d1de-a9ad-4728-9018-5b2db3b987c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = inference(pairs, model, device, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23114c85-5eed-42cf-839a-3ff9a996c461",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ce266b-96e7-4672-9c03-a8f48872e9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in output.items():\n",
    "#     print(k)\n",
    "#     if type(v) == dict:\n",
    "#         for k, v in v.items():\n",
    "#             try:\n",
    "#                 print(f\"   {k} shape: {v.shape}\")\n",
    "#             except Exception as e:\n",
    "#                 print(f\"   {k}: {v}\")i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9480ff5d-bd85-4815-a3ab-c061717b6875",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(output.keys()) == [\"view1\", \"view2\", \"pred1\", \"pred2\", \"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898531d-5959-483b-849b-31a7ca414886",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(output[\"view1\"].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"]\n",
    "assert output[\"view1\"][\"img\"].shape == torch.Size([2, 3, 384, 512])\n",
    "assert output[\"view1\"][\"true_shape\"].shape == torch.Size([2, 2])\n",
    "assert np.all(output[\"view1\"][\"idx\"] == np.array([1, 0]))\n",
    "assert output[\"view1\"][\"instance\"] == [\"1\", \"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa20d593-d868-476c-81ac-863eee0f9538",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(output[\"view2\"].keys()) == [\"img\", \"true_shape\", \"idx\", \"instance\"]\n",
    "assert output[\"view2\"][\"img\"].shape == torch.Size([2, 3, 384, 512])\n",
    "assert output[\"view2\"][\"true_shape\"].shape == torch.Size([2, 2])\n",
    "assert np.all(output[\"view2\"][\"idx\"] == np.array([0, 1]))\n",
    "assert output[\"view2\"][\"instance\"] == [\"0\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b445e-f151-48ea-ad71-cfa0f8bd95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_0_idx = images[0][\"idx\"]\n",
    "image_1_idx = images[1][\"idx\"]\n",
    "image_0_idx_for_view_1_output = output[\"view1\"][\"idx\"].index(image_0_idx)\n",
    "image_1_idx_for_view_1_output = output[\"view1\"][\"idx\"].index(image_1_idx)\n",
    "image_0_idx_for_view_2_output = output[\"view2\"][\"idx\"].index(image_0_idx)\n",
    "image_1_idx_for_view_2_output = output[\"view2\"][\"idx\"].index(image_1_idx)\n",
    "image_0_for_view_1_output = output[\"view1\"][\"img\"][image_0_idx_for_view_1_output]\n",
    "image_0_for_view_2_output = output[\"view2\"][\"img\"][image_0_idx_for_view_2_output]\n",
    "image_1_for_view_1_output = output[\"view1\"][\"img\"][image_1_idx_for_view_1_output]\n",
    "image_1_for_view_2_output = output[\"view2\"][\"img\"][image_1_idx_for_view_2_output]\n",
    "\n",
    "assert torch.all(image_0_for_view_1_output == image_0_for_view_2_output)\n",
    "assert torch.all(image_1_for_view_1_output == image_1_for_view_2_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec9452-0225-4c8a-86e0-93d6aa439ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(output[\"pred1\"].keys()) == [\"pts3d\", \"conf\"]\n",
    "assert output[\"pred1\"][\"pts3d\"].shape == torch.Size([2, 384, 512, 3])\n",
    "assert output[\"pred1\"][\"conf\"].shape == torch.Size([2, 384, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6eb022",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert list(output[\"pred2\"].keys()) == [\"conf\", \"pts3d_in_other_view\"]\n",
    "assert output[\"pred2\"][\"pts3d_in_other_view\"].shape == torch.Size([2, 384, 512, 3])\n",
    "assert output[\"pred2\"][\"conf\"].shape == torch.Size([2, 384, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a791c-6216-4ea9-9c5c-f9c9273cf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert output[\"loss\"] == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accf4416-85ba-4f9b-93f5-f8d7b351f9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this stage, you have the raw dust3r predictions\n",
    "view1, pred1 = output[\"view1\"], output[\"pred1\"]\n",
    "view2, pred2 = output[\"view2\"], output[\"pred2\"]\n",
    "# here, view1, pred1, view2, pred2 are dicts of lists of len(2)\n",
    "#  -> because we symmetrize we have (im1, im2) and (im2, im1) pairs\n",
    "# in each view you have:\n",
    "# an integer image identifier: view1['idx'] and view2['idx']\n",
    "# the img: view1['img'] and view2['img']\n",
    "# the image shape: view1['true_shape'] and view2['true_shape']\n",
    "# an instance string output by the dataloader: view1['instance'] and view2['instance']\n",
    "# pred1 and pred2 contains the confidence values: pred1['conf'] and pred2['conf']\n",
    "# pred1 contains 3D points for view1['img'] in view1['img'] space: pred1['pts3d']\n",
    "# pred2 contains 3D points for view2['img'] in view1['img'] space: pred2['pts3d_in_other_view']\n",
    "\n",
    "# next we'll use the global_aligner to align the predictions\n",
    "# depending on your task, you may be fine with the raw output and not need it\n",
    "# with only two input images, you could use GlobalAlignerMode.PairViewer: it would just convert the output\n",
    "# if using GlobalAlignerMode.PairViewer, no need to run compute_global_alignment\n",
    "scene = global_aligner(output, device=device, mode=GlobalAlignerMode.PointCloudOptimizer)\n",
    "loss = scene.compute_global_alignment(init=\"mst\", niter=niter, schedule=schedule, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74543f2c-9535-4423-a428-c6180efab2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(scene.imgs) == 2\n",
    "assert scene.imgs[0].shape == (384, 512, 3)\n",
    "assert len(scene.get_pts3d()) == 2\n",
    "assert scene.get_pts3d()[0].shape == torch.Size([384, 512, 3])\n",
    "assert len(scene.get_masks()) == 2\n",
    "assert scene.get_masks()[0].shape == torch.Size([384, 512])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ce3433-9f98-48ae-ae13-26770ae26724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve useful values from scene:\n",
    "imgs = scene.imgs\n",
    "focals = scene.get_focals()\n",
    "poses = scene.get_im_poses()\n",
    "pts3d = scene.get_pts3d()\n",
    "confidence_masks = scene.get_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc401e3e-6e69-4f3f-9ca8-db81a8b6621d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize reconstruction\n",
    "scene.show(viewer=\"gl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1536796-3759-46f1-bdac-10f10c348dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 2D-2D matches between the two images\n",
    "from dust3r.utils.geometry import find_reciprocal_matches, xy_grid\n",
    "\n",
    "pts2d_list, pts3d_list = [], []\n",
    "for i in range(2):\n",
    "    conf_i = confidence_masks[i].cpu().numpy()\n",
    "    pts2d_list.append(xy_grid(*imgs[i].shape[:2][::-1])[conf_i])  # imgs[i].shape[:2] = (H, W)\n",
    "    pts3d_list.append(pts3d[i].detach().cpu().numpy()[conf_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35ef89a-1348-4540-9eef-50175ed0e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert imgs[0].shape[:2][::-1] == (512, 384)\n",
    "\n",
    "assert confidence_masks[0].cpu().numpy().shape == (384, 512)\n",
    "assert xy_grid(*imgs[0].shape[:2][::-1]).shape == (384, 512, 2)\n",
    "assert pts3d[0].detach().cpu().numpy().shape == (384, 512, 3)\n",
    "\n",
    "# number of points selected for 2d should be the same as the mask where pixels are True\n",
    "assert np.sum(confidence_masks[0].cpu().numpy()) == len(pts2d_list[0])\n",
    "assert np.sum(confidence_masks[0].cpu().numpy()) == len(pts3d_list[0])\n",
    "\n",
    "assert imgs[1].shape[:2][::-1] == (512, 384)\n",
    "\n",
    "assert confidence_masks[1].cpu().numpy().shape == (384, 512)\n",
    "assert xy_grid(*imgs[1].shape[:2][::-1]).shape == (384, 512, 2)\n",
    "assert pts3d[1].detach().cpu().numpy().shape == (384, 512, 3)\n",
    "\n",
    "# number of points selected for 2d should be the same as the mask where pixels are True\n",
    "assert np.sum(confidence_masks[1].cpu().numpy()) == len(pts2d_list[1])\n",
    "assert np.sum(confidence_masks[1].cpu().numpy()) == len(pts3d_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f94393-235c-400d-8d32-e0a19ba23647",
   "metadata": {},
   "outputs": [],
   "source": [
    "reciprocal_in_P2, nn2_in_P1, num_matches = find_reciprocal_matches(*pts3d_list)\n",
    "print(f\"found {num_matches} matches\")\n",
    "matches_im1 = pts2d_list[1][reciprocal_in_P2]\n",
    "matches_im0 = pts2d_list[0][nn2_in_P1][reciprocal_in_P2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf40742e-084f-4d1f-961a-843618a064d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize a few matches\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as pl\n",
    "\n",
    "n_viz = 10\n",
    "match_idx_to_viz = np.round(np.linspace(0, num_matches - 1, n_viz)).astype(int)\n",
    "viz_matches_im0, viz_matches_im1 = (\n",
    "    matches_im0[match_idx_to_viz],\n",
    "    matches_im1[match_idx_to_viz],\n",
    ")\n",
    "\n",
    "H0, W0, H1, W1 = *imgs[0].shape[:2], *imgs[1].shape[:2]\n",
    "img0 = np.pad(imgs[0], ((0, max(H1 - H0, 0)), (0, 0), (0, 0)), \"constant\", constant_values=0)\n",
    "img1 = np.pad(imgs[1], ((0, max(H0 - H1, 0)), (0, 0), (0, 0)), \"constant\", constant_values=0)\n",
    "img = np.concatenate((img0, img1), axis=1)\n",
    "pl.figure()\n",
    "pl.imshow(img)\n",
    "cmap = pl.get_cmap(\"jet\")\n",
    "for i in range(n_viz):\n",
    "    (x0, y0), (x1, y1) = viz_matches_im0[i].T, viz_matches_im1[i].T\n",
    "    pl.plot(\n",
    "        [x0, x1 + W0],\n",
    "        [y0, y1],\n",
    "        \"-+\",\n",
    "        color=cmap(i / (n_viz - 1)),\n",
    "        scalex=False,\n",
    "        scaley=False,\n",
    "    )\n",
    "pl.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f1189-188c-4985-a892-130a45dc0596",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotated-dust3r",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
