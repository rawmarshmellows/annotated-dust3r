{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "e109ec74-bb47-439b-bffc-d4d318cf76d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "c507a5b0-e768-4e17-8f61-c9cdb148fa30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, cannot find cuda-compiled version of RoPE2D, using a slow pytorch version instead\n"
     ]
    }
   ],
   "source": [
    "from src.vendored.croco.models.croco import CroCoNet\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "de6a3c6b-8a25-4335-8837-c10fecd6a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p pretrained_models/\n",
    "# !wget https://download.europe.naverlabs.com/ComputerVision/CroCo/CroCo.pth -P pretrained_models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "a5d91947-0fe7-40a7-bd53-cc74b51ce38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2bf4d9-5e1d-4923-80a7-b42a5697a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() and torch.cuda.device_count() > 0 else \"cpu\")\n",
    "\n",
    "# load 224x224 images and transform them to tensor\n",
    "imagenet_mean = [0.485, 0.456, 0.406]\n",
    "imagenet_mean_tensor = torch.tensor(imagenet_mean).view(1, 3, 1, 1).to(device, non_blocking=True)\n",
    "imagenet_std = [0.229, 0.224, 0.225]\n",
    "imagenet_std_tensor = torch.tensor(imagenet_std).view(1, 3, 1, 1).to(device, non_blocking=True)\n",
    "trfs = Compose([ToTensor(), Normalize(mean=imagenet_mean, std=imagenet_std)])\n",
    "image1 = trfs(Image.open(\"../assets/Chateau1.png\").convert(\"RGB\")).to(device, non_blocking=True).unsqueeze(0)\n",
    "image2 = trfs(Image.open(\"../assets/Chateau2.png\").convert(\"RGB\")).to(device, non_blocking=True).unsqueeze(0)\n",
    "\n",
    "# load model\n",
    "ckpt = torch.load(Path.cwd() / \"pretrained_models/CroCo.pth\", \"cpu\")\n",
    "model = CroCoNet(**ckpt.get(\"croco_kwargs\", {})).to(device)\n",
    "model.eval()\n",
    "msg = model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "# forward\n",
    "with torch.inference_mode():\n",
    "    out, mask, target = model(image1, image2)\n",
    "\n",
    "# the output is normalized, thus use the mean/std of the actual image to go back to RGB space\n",
    "patchified = model.patchify(image1)\n",
    "mean = patchified.mean(dim=-1, keepdim=True)\n",
    "var = patchified.var(dim=-1, keepdim=True)\n",
    "decoded_image = model.unpatchify(out * (var + 1.0e-6) ** 0.5 + mean)\n",
    "# undo imagenet normalization, prepare masked image\n",
    "decoded_image = decoded_image * imagenet_std_tensor + imagenet_mean_tensor\n",
    "input_image = image1 * imagenet_std_tensor + imagenet_mean_tensor\n",
    "ref_image = image2 * imagenet_std_tensor + imagenet_mean_tensor\n",
    "image_masks = model.unpatchify(model.patchify(torch.ones_like(ref_image)) * mask[:, :, None])\n",
    "masked_input_image = (1 - image_masks) * input_image\n",
    "\n",
    "# make visualization\n",
    "visualization = torch.cat(\n",
    "    (ref_image, masked_input_image, decoded_image, input_image), dim=3\n",
    ")  # 4*(B, 3, H, W) -> B, 3, H, W*4\n",
    "B, C, H, W = visualization.shape\n",
    "visualization = visualization.permute(1, 0, 2, 3).reshape(C, B * H, W)\n",
    "visualization = torchvision.transforms.functional.to_pil_image(torch.clamp(visualization, 0, 1))\n",
    "fname = \"demo_output.png\"\n",
    "visualization.save(fname)\n",
    "print(\"Visualization save in \" + fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9842fe-af8d-4931-b5e6-f645d33a0735",
   "metadata": {},
   "source": [
    "### Patchify/Unpatchify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ecb0a1-c717-4e83-9bd4-bdf6ff3f5d4a",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffd5e39-1209-4cfb-a5b5-7eb578dbf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def patchify(imgs, patch_size):\n",
    "    \"\"\"\n",
    "    Divide images into non-overlapping square patches.\n",
    "\n",
    "    Parameters:\n",
    "    - imgs (torch.Tensor): Input images of shape (B, C, H, W).\n",
    "    - patch_size (int): Size of each square patch (p).\n",
    "\n",
    "    Returns:\n",
    "    - patches (torch.Tensor): Patches of shape (B, L, p^2 * C),\n",
    "                              where L = (H // p) * (W // p).\n",
    "    - num_patches_h (int): Number of patches along the height.\n",
    "    - num_patches_w (int): Number of patches along the width.\n",
    "    \"\"\"\n",
    "    B, C, H, W = imgs.shape\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "    num_patches_h = H // patch_size\n",
    "    num_patches_w = W // patch_size\n",
    "\n",
    "    # Reshape to (B, C, num_patches_h, patch_size, num_patches_w, patch_size)\n",
    "    x = imgs.reshape(B, C, num_patches_h, patch_size, num_patches_w, patch_size)\n",
    "\n",
    "    # Permute to (B, num_patches_h, num_patches_w, patch_size, patch_size, C)\n",
    "    x = x.permute(0, 2, 4, 3, 5, 1)\n",
    "\n",
    "    # Reshape to (B, L, p^2 * C), where L = num_patches_h * num_patches_w\n",
    "    patches = x.reshape(B, num_patches_h * num_patches_w, patch_size * patch_size * C)\n",
    "\n",
    "    return patches, num_patches_h, num_patches_w\n",
    "\n",
    "\n",
    "def unpatchify(patches, patch_size, num_patches_h, num_patches_w, channels=3):\n",
    "    \"\"\"\n",
    "    Reconstruct images from patches.\n",
    "\n",
    "    Parameters:\n",
    "    - patches (torch.Tensor): Patches of shape (B, L, p^2 * C),\n",
    "                              where L = (H // p) * (W // p).\n",
    "    - patch_size (int): Size of each square patch (p).\n",
    "    - num_patches_h (int): Number of patches along the height.\n",
    "    - num_patches_w (int): Number of patches along the width.\n",
    "    - channels (int): Number of channels in the image (default: 3).\n",
    "\n",
    "    Returns:\n",
    "    - imgs (torch.Tensor): Reconstructed images of shape (B, C, H, W).\n",
    "    \"\"\"\n",
    "    B, L, patch_dim = patches.shape\n",
    "    assert patch_dim % channels == 0, \"Patch dimension is not compatible with the number of channels.\"\n",
    "\n",
    "    p_squared = patch_dim // channels\n",
    "    p = int(patch_size)\n",
    "    assert p * p == p_squared, \"Patch size does not match patch dimension.\"\n",
    "\n",
    "    expected_L = num_patches_h * num_patches_w\n",
    "    assert L == expected_L, f\"Number of patches (L={L}) does not match num_patches_h * num_patches_w ({expected_L}).\"\n",
    "\n",
    "    # Reshape to (B, num_patches_h, num_patches_w, patch_size, patch_size, C)\n",
    "    x = patches.reshape(B, num_patches_h, num_patches_w, patch_size, patch_size, channels)\n",
    "\n",
    "    # Permute to (B, C, num_patches_h, patch_size, num_patches_w, patch_size)\n",
    "    x = x.permute(0, 5, 1, 3, 2, 4)\n",
    "\n",
    "    # Reshape to (B, C, H, W), where H = num_patches_h * patch_size, W = num_patches_w * patch_size\n",
    "    H = num_patches_h * patch_size\n",
    "    W = num_patches_w * patch_size\n",
    "    imgs = x.reshape(B, channels, H, W)\n",
    "\n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d76f0cd-4d95-4abd-8235-f85f56a21134",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7809659f-a76f-4f5a-8f50-cf8ec1096fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_patchify_unpatchify_round_trip():\n",
    "    \"\"\"\n",
    "    Test that unpatchify(patchify(imgs)) == imgs for random tensors.\n",
    "    \"\"\"\n",
    "    print(\"Running Round-Trip Test...\")\n",
    "    B, C, H, W = 2, 3, 128, 128  # Square images\n",
    "    patch_size = 32\n",
    "    imgs = torch.randn(B, C, H, W)\n",
    "\n",
    "    patches, num_patches_h, num_patches_w = patchify(imgs, patch_size)\n",
    "    reconstructed = unpatchify(patches, patch_size, num_patches_h, num_patches_w, channels=C)\n",
    "\n",
    "    assert torch.allclose(\n",
    "        imgs, reconstructed, atol=1e-6\n",
    "    ), \"Round-trip patchify -> unpatchify failed for square images.\"\n",
    "    print(\"Round-Trip Test Passed for Square Images.\")\n",
    "\n",
    "\n",
    "def test_patchify_unpatchify_specific_case():\n",
    "    \"\"\"\n",
    "    Test patchify and unpatchify with a specific tensor where the outcome is known.\n",
    "    \"\"\"\n",
    "    print(\"Running Specific Case Test...\")\n",
    "    # Create a simple tensor where each pixel value is unique\n",
    "    B, C, H, W = 1, 1, 4, 6  # Non-square image\n",
    "    patch_size = 2\n",
    "    imgs = torch.arange(B * C * H * W).reshape(B, C, H, W).float()\n",
    "\n",
    "    # Expected patches:\n",
    "    # For H=4, W=6, patch_size=2 -> num_patches_h=2, num_patches_w=3\n",
    "    # Patches are ordered row-wise\n",
    "    # Patch 1: [[0, 1], [6, 7]]\n",
    "    # Patch 2: [[2, 3], [8, 9]]\n",
    "    # Patch 3: [[4, 5], [10,11]]\n",
    "    # Patch 4: [[12,13], [18,19]]\n",
    "    # Patch 5: [[14,15], [20,21]]\n",
    "    # Patch 6: [[16,17], [22,23]]\n",
    "    expected_patches = torch.tensor(\n",
    "        [\n",
    "            [\n",
    "                [0.0, 1.0, 6.0, 7.0],\n",
    "                [2.0, 3.0, 8.0, 9.0],\n",
    "                [4.0, 5.0, 10.0, 11.0],\n",
    "                [12.0, 13.0, 18.0, 19.0],\n",
    "                [14.0, 15.0, 20.0, 21.0],\n",
    "                [16.0, 17.0, 22.0, 23.0],\n",
    "            ]\n",
    "        ]\n",
    "    )  # Shape: (1, 6, 4)\n",
    "\n",
    "    patches, num_patches_h, num_patches_w = patchify(imgs, patch_size)\n",
    "    assert torch.allclose(patches, expected_patches, atol=1e-6), \"Patchify specific case failed.\"\n",
    "    print(\"Patchify Specific Case Test Passed.\")\n",
    "\n",
    "    # Now test unpatchify\n",
    "    reconstructed = unpatchify(patches, patch_size, num_patches_h, num_patches_w, channels=C)\n",
    "    assert torch.allclose(imgs, reconstructed, atol=1e-6), \"Unpatchify specific case failed.\"\n",
    "    print(\"Unpatchify Specific Case Test Passed.\")\n",
    "\n",
    "\n",
    "def test_patchify_unpatchify_non_square():\n",
    "    \"\"\"\n",
    "    Test patchify and unpatchify with non-square images.\n",
    "    \"\"\"\n",
    "    print(\"Running Non-Square Image Test...\")\n",
    "    B, C, H, W = 1, 3, 64, 32  # Non-square image\n",
    "    patch_size = 16\n",
    "    imgs = torch.randn(B, C, H, W)\n",
    "\n",
    "    patches, num_patches_h, num_patches_w = patchify(imgs, patch_size)\n",
    "    reconstructed = unpatchify(patches, patch_size, num_patches_h, num_patches_w, channels=C)\n",
    "\n",
    "    assert torch.allclose(imgs, reconstructed, atol=1e-6), \"Patchify -> Unpatchify failed for non-square images.\"\n",
    "    print(\"Non-Square Image Test Passed.\")\n",
    "\n",
    "\n",
    "def test_invalid_inputs():\n",
    "    \"\"\"\n",
    "    Test that the functions handle invalid inputs gracefully.\n",
    "    \"\"\"\n",
    "    print(\"Running Invalid Input Test...\")\n",
    "    # Image dimensions not divisible by patch_size\n",
    "    B, C, H, W = 1, 3, 65, 33  # Not divisible by 16\n",
    "    patch_size = 16\n",
    "    imgs = torch.randn(B, C, H, W)\n",
    "\n",
    "    try:\n",
    "        patches, num_patches_h, num_patches_w = patchify(imgs, patch_size)\n",
    "    except AssertionError as e:\n",
    "        print(f\"Properly caught invalid input: {e}\")\n",
    "    else:\n",
    "        assert False, \"Failed to catch invalid input where H and W are not divisible by patch_size.\"\n",
    "\n",
    "    # Unpatchify with incorrect number of patches\n",
    "    B, C, H, W = 1, 3, 64, 32\n",
    "    patch_size = 16\n",
    "    imgs = torch.randn(B, C, H, W)\n",
    "    patches, num_patches_h, num_patches_w = patchify(imgs, patch_size)\n",
    "\n",
    "    # Tamper with patches\n",
    "    patches_tampered = patches[:, :-1, :]\n",
    "\n",
    "    try:\n",
    "        reconstructed = unpatchify(patches_tampered, patch_size, num_patches_h, num_patches_w, channels=C)\n",
    "    except AssertionError as e:\n",
    "        print(f\"Properly caught unpatchify with incorrect number of patches: {e}\")\n",
    "    else:\n",
    "        assert False, \"Failed to catch unpatchify with incorrect number of patches.\"\n",
    "\n",
    "    print(\"Invalid Input Test Passed.\")\n",
    "\n",
    "\n",
    "def run_all_tests():\n",
    "    \"\"\"\n",
    "    Run all defined tests.\n",
    "    \"\"\"\n",
    "    test_patchify_unpatchify_round_trip()\n",
    "    test_patchify_unpatchify_specific_case()\n",
    "    test_patchify_unpatchify_non_square()\n",
    "    test_invalid_inputs()\n",
    "    print(\"All Tests Passed Successfully.\")\n",
    "\n",
    "\n",
    "run_all_tests()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7979c4b4-b9cd-4d7d-bee8-960a5e60056a",
   "metadata": {},
   "source": [
    "### Patch Embed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c43e986-6842-4f00-8cff-e89de9730f1c",
   "metadata": {},
   "source": [
    "#### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe71ce4-4883-468a-8e25-686b76bb4b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PositionGetter:\n",
    "    \"\"\"\n",
    "    Generates and caches patch position encodings for image patches.\n",
    "\n",
    "    This class creates position encodings for image patches in a grid layout,\n",
    "    caching them for reuse to improve efficiency when the same grid dimensions\n",
    "    are requested multiple times.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize an empty cache for position encodings.\"\"\"\n",
    "        self.cache_positions: Dict[Tuple[int, int], torch.Tensor] = {}\n",
    "\n",
    "    def __call__(self, batch_size: int, num_patches_h: int, num_patches_w: int, device: torch.device) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate or retrieve cached position encodings for the specified patch grid.\n",
    "\n",
    "        Parameters:\n",
    "            batch_size (int): Number of samples in the batch\n",
    "            num_patches_h (int): Number of patches in height dimension\n",
    "            num_patches_w (int): Number of patches in width dimension\n",
    "            device (torch.device): Device to place the tensors on\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Position encodings of shape (batch_size, num_patches_h * num_patches_w, 2)\n",
    "        \"\"\"\n",
    "        if (num_patches_h, num_patches_w) not in self.cache_positions:\n",
    "            self.cache_positions[num_patches_h, num_patches_w] = self._generate_patch_positions_with_dimension(\n",
    "                num_patches_h, num_patches_w, device\n",
    "            )\n",
    "        return self._expand_patch_positions_with_batch_size(\n",
    "            self.cache_positions[num_patches_h, num_patches_w], batch_size\n",
    "        )\n",
    "\n",
    "    def _generate_patch_positions_with_dimension(\n",
    "        self, num_patches_h: int, num_patches_w: int, device: torch.device\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate position encodings for a single sample.\n",
    "\n",
    "        Parameters:\n",
    "            num_patches_h (int): Number of patches in height dimension\n",
    "            num_patches_w (int): Number of patches in width dimension\n",
    "            device (torch.device): Device to place the tensors on\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Position encodings of shape (num_patches_h * num_patches_w, 2)\n",
    "        \"\"\"\n",
    "        y = torch.arange(num_patches_h, device=device)\n",
    "        x = torch.arange(num_patches_w, device=device)\n",
    "        positions_for_patch = torch.cartesian_prod(y, x)\n",
    "        return positions_for_patch\n",
    "\n",
    "    def _expand_patch_positions_with_batch_size(self, patch_positions: torch.Tensor, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Expand position encodings to match batch size.\n",
    "\n",
    "        Parameters:\n",
    "            patch_positions (torch.Tensor): Position encodings for a single sample\n",
    "            batch_size (int): Number of samples in the batch\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Position encodings of shape (batch_size, num_patches_h * num_patches_w, 2)\n",
    "        \"\"\"\n",
    "        num_positions = patch_positions.size(0)\n",
    "        unsqueezed_patch_positions = patch_positions.unsqueeze(0)\n",
    "        expanded_patch_positions_with_batch_size = unsqueezed_patch_positions.expand(batch_size, num_positions, 2)\n",
    "        return expanded_patch_positions_with_batch_size\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds image patches into a specified embedding dimension.\n",
    "\n",
    "    This module splits an image into non-overlapping patches, projects each patch\n",
    "    into a high-dimensional space using a convolutional layer, applies normalization,\n",
    "    and provides positional encodings for each patch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        in_channels: int = 3,\n",
    "        embed_dim: int = 768,\n",
    "        norm_layer: nn.Module = None,\n",
    "        flatten: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the PatchEmbed module.\n",
    "\n",
    "        Parameters:\n",
    "            img_size (int): Size of the input image (assumed square)\n",
    "            patch_size (int): Size of each patch (assumed square)\n",
    "            in_channels (int): Number of input image channels\n",
    "            embed_dim (int): Dimension of the patch embeddings\n",
    "            norm_layer (nn.Module, optional): Normalization layer constructor\n",
    "            flatten (bool): If True, flatten spatial dimensions after projection\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Check compatibility of norm_layer and flatten settings\n",
    "        if not flatten and norm_layer is not None and norm_layer != nn.Identity:\n",
    "            raise ValueError(\n",
    "                \"LayerNorm cannot be used with flatten=False. \"\n",
    "                \"When flatten=False, the output shape is (B, embed_dim, H, W) \"\n",
    "                \"which is incompatible with LayerNorm's expected shape (B, H*W, embed_dim).\"\n",
    "            )\n",
    "\n",
    "        self.img_size = (img_size, img_size)\n",
    "        self.patch_size = (patch_size, patch_size)\n",
    "\n",
    "        self.num_patches_h = self.img_size[0] // self.patch_size[0]\n",
    "        self.num_patches_w = self.img_size[1] // self.patch_size[1]\n",
    "        self.num_patches = self.num_patches_h * self.num_patches_w\n",
    "\n",
    "        self.flatten = flatten\n",
    "\n",
    "        # Project patches to embedding dimension\n",
    "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "        self.position_getter = PositionGetter()\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize the weights using Xavier uniform initialization.\"\"\"\n",
    "        nn.init.xavier_uniform_(self.proj.weight)\n",
    "        if self.proj.bias is not None:\n",
    "            nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Process input images through the patch embedding module.\n",
    "\n",
    "        Parameters:\n",
    "            x (torch.Tensor): Input images of shape (B, C, H, W)\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]:\n",
    "                - Patch embeddings of shape:\n",
    "                    - If flatten=True: (B, num_patches, embed_dim)\n",
    "                    - If flatten=False: (B, embed_dim, num_patches_h, num_patches_w)\n",
    "                - Position encodings of shape (B, num_patches, 2)\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        assert (\n",
    "            H == self.img_size[0]\n",
    "        ), f\"Input image height ({H}) doesn't match model's expected height ({self.img_size[0]}).\"\n",
    "        assert (\n",
    "            W == self.img_size[1]\n",
    "        ), f\"Input image width ({W}) doesn't match model's expected width ({self.img_size[1]}).\"\n",
    "\n",
    "        x = self.proj(x)  # (B, embed_dim, num_patches_h, num_patches_w)\n",
    "\n",
    "        pos_encodings = self.position_getter(\n",
    "            batch_size=B, num_patches_h=self.num_patches_h, num_patches_w=self.num_patches_w, device=x.device\n",
    "        )  # (B, num_patches, 2)\n",
    "\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        return x, pos_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9449a2-0218-4615-9d1d-5c59b98eb0ea",
   "metadata": {},
   "source": [
    "#### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e860d-0848-42f1-ab20-7116d477bd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def test_position_getter():\n",
    "    \"\"\"Test the PositionGetter class.\"\"\"\n",
    "    print(\"Testing PositionGetter...\")\n",
    "\n",
    "    # Test initialization\n",
    "    pos_getter = PositionGetter()\n",
    "    assert isinstance(pos_getter.cache_positions, dict), \"Cache should be a dictionary\"\n",
    "    assert len(pos_getter.cache_positions) == 0, \"Cache should be empty at initialization\"\n",
    "    print(\"✓ Initialization test passed\")\n",
    "\n",
    "    # Test position generation and caching\n",
    "    batch_size = 2\n",
    "    num_patches_h = 3\n",
    "    num_patches_w = 4\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Generate positions\n",
    "    positions = pos_getter(batch_size, num_patches_h, num_patches_w, device)\n",
    "\n",
    "    # Check shape\n",
    "    assert positions.shape == (\n",
    "        batch_size,\n",
    "        num_patches_h * num_patches_w,\n",
    "        2,\n",
    "    ), f\"Expected shape ({batch_size}, {num_patches_h * num_patches_w}, 2), got {positions.shape}\"\n",
    "\n",
    "    # Check cache\n",
    "    cache_key = (num_patches_h, num_patches_w)\n",
    "    assert cache_key in pos_getter.cache_positions, \"Positions should be cached\"\n",
    "    cached_positions = pos_getter.cache_positions[cache_key]\n",
    "    assert cached_positions.shape == (\n",
    "        num_patches_h * num_patches_w,\n",
    "        2,\n",
    "    ), \"Cached positions should have shape (num_patches_h * num_patches_w, 2)\"\n",
    "\n",
    "    # Check coordinate values\n",
    "    assert torch.all(positions[0, 0] == torch.tensor([0, 0], device=device)), \"First position should be (0, 0)\"\n",
    "    assert torch.all(\n",
    "        positions[0, -1] == torch.tensor([num_patches_h - 1, num_patches_w - 1], device=device)\n",
    "    ), f\"Last position should be ({num_patches_h-1}, {num_patches_w-1})\"\n",
    "    print(\"✓ Position generation test passed\")\n",
    "\n",
    "    # Test cache reuse\n",
    "    positions2 = pos_getter(batch_size, num_patches_h, num_patches_w, device)\n",
    "    assert torch.equal(positions, positions2), \"Cached positions should be identical\"\n",
    "    assert len(pos_getter.cache_positions) == 1, \"Cache should have only one entry\"\n",
    "    print(\"✓ Cache reuse test passed\")\n",
    "\n",
    "    print(\"All PositionGetter tests passed!\\n\")\n",
    "\n",
    "\n",
    "def test_patch_embed():\n",
    "    \"\"\"Test the PatchEmbed class.\"\"\"\n",
    "    print(\"Testing PatchEmbed...\")\n",
    "\n",
    "    # Test valid initialization with flatten=True and LayerNorm\n",
    "    patch_embed = PatchEmbed(\n",
    "        img_size=224, patch_size=16, in_channels=3, embed_dim=768, norm_layer=nn.LayerNorm, flatten=True\n",
    "    )\n",
    "    print(\"✓ Valid initialization with flatten=True and LayerNorm passed\")\n",
    "\n",
    "    # Test initialization with flatten=False and no norm\n",
    "    patch_embed_no_norm = PatchEmbed(\n",
    "        img_size=224, patch_size=16, in_channels=3, embed_dim=768, norm_layer=None, flatten=False\n",
    "    )\n",
    "    print(\"✓ Valid initialization with flatten=False and no norm passed\")\n",
    "\n",
    "    # Test initialization with flatten=False and Identity norm\n",
    "    patch_embed_identity = PatchEmbed(\n",
    "        img_size=224, patch_size=16, in_channels=3, embed_dim=768, norm_layer=nn.Identity, flatten=False\n",
    "    )\n",
    "    print(\"✓ Valid initialization with flatten=False and Identity norm passed\")\n",
    "\n",
    "    # Test that LayerNorm with flatten=False raises error\n",
    "    try:\n",
    "        patch_embed_invalid = PatchEmbed(\n",
    "            img_size=224, patch_size=16, in_channels=3, embed_dim=768, norm_layer=nn.LayerNorm, flatten=False\n",
    "        )\n",
    "        assert False, \"Should have raised ValueError\"\n",
    "    except ValueError as e:\n",
    "        assert \"LayerNorm cannot be used with flatten=False\" in str(\n",
    "            e\n",
    "        ), \"Should raise specific error about LayerNorm incompatibility\"\n",
    "    print(\"✓ LayerNorm incompatibility check passed\")\n",
    "\n",
    "    # Test forward pass with flatten=True\n",
    "    batch_size = 4\n",
    "    x = torch.randn(batch_size, 3, 224, 224)\n",
    "\n",
    "    embeddings, pos_encodings = patch_embed(x)\n",
    "    assert embeddings.shape == (\n",
    "        batch_size,\n",
    "        196,\n",
    "        768,\n",
    "    ), f\"Expected embeddings shape (4, 196, 768), got {embeddings.shape}\"\n",
    "    assert pos_encodings.shape == (\n",
    "        batch_size,\n",
    "        196,\n",
    "        2,\n",
    "    ), f\"Expected positions shape (4, 196, 2), got {pos_encodings.shape}\"\n",
    "    print(\"✓ Forward pass with flatten=True test passed\")\n",
    "\n",
    "    # Test forward pass with flatten=False\n",
    "    embeddings, pos_encodings = patch_embed_no_norm(x)\n",
    "    assert embeddings.shape == (\n",
    "        batch_size,\n",
    "        768,\n",
    "        14,\n",
    "        14,\n",
    "    ), f\"Expected embeddings shape (4, 768, 14, 14), got {embeddings.shape}\"\n",
    "    assert pos_encodings.shape == (\n",
    "        batch_size,\n",
    "        196,\n",
    "        2,\n",
    "    ), f\"Expected positions shape (4, 196, 2), got {pos_encodings.shape}\"\n",
    "    print(\"✓ Forward pass with flatten=False test passed\")\n",
    "\n",
    "    # Test invalid input size\n",
    "    try:\n",
    "        x_invalid = torch.randn(batch_size, 3, 256, 224)\n",
    "        patch_embed(x_invalid)\n",
    "        assert False, \"Should have raised AssertionError\"\n",
    "    except AssertionError as e:\n",
    "        assert \"Input image height\" in str(e), \"Should raise error about invalid input height\"\n",
    "    print(\"✓ Invalid input size check passed\")\n",
    "\n",
    "    # Test weight initialization\n",
    "    def check_xavier_uniform(tensor):\n",
    "        fan_in = tensor.size(1)\n",
    "        fan_out = tensor.size(0)\n",
    "        bound = np.sqrt(6.0 / (fan_in + fan_out))\n",
    "        return torch.all(tensor >= -bound) and torch.all(tensor <= bound)\n",
    "\n",
    "    w = patch_embed.proj.weight\n",
    "    assert check_xavier_uniform(w.view(w.size(0), -1)), \"Weights should follow Xavier uniform distribution\"\n",
    "\n",
    "    if patch_embed.proj.bias is not None:\n",
    "        assert torch.all(patch_embed.proj.bias == 0), \"Bias should be initialized to zero\"\n",
    "    print(\"✓ Weight initialization test passed\")\n",
    "\n",
    "    print(\"All PatchEmbed tests passed!\\n\")\n",
    "\n",
    "\n",
    "test_position_getter()\n",
    "test_patch_embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a24115-3684-4102-a305-baee27b6705a",
   "metadata": {},
   "source": [
    "### Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f66c5d5-61e1-4452-b572-8b36f5bda7c4",
   "metadata": {},
   "source": [
    "#### Implementation 1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec77a4d8-ec3d-488a-82f4-f1f0c66823a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_1d_sincos_pos_embed_from_grid(embed_dim: int, coordinate_grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional embeddings for 1D positions.\n",
    "\n",
    "    This function creates embeddings where each dimension corresponds to a sinusoid\n",
    "    of a different frequency. The first half uses sine, the second half uses cosine.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Dimension of the output embeddings (must be even)\n",
    "        positions: Array of positions to encode, will be flattened\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Position embeddings with shape [len(flattened_positions), embed_dim]\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Generate frequency bands for the sinusoidal embeddings\n",
    "    # Each position will be encoded with sinusoids of these frequencies\n",
    "    frequency_bands = np.arange(embed_dim // 2, dtype=float)\n",
    "    frequency_bands /= embed_dim / 2.0\n",
    "    frequency_bands = 1.0 / 10000**frequency_bands  # Shape: (D/2,)\n",
    "\n",
    "    # Flatten input positions\n",
    "    flattened_positions = coordinate_grid.reshape(-1)  # Shape: (M,)\n",
    "\n",
    "    # Compute position-frequency products\n",
    "    # This creates a matrix where each row corresponds to a position and\n",
    "    # each column corresponds to that position multiplied by a frequency\n",
    "    phase_matrix = np.einsum(\"m,d->md\", flattened_positions, frequency_bands)  # Shape: (M, D/2)\n",
    "\n",
    "    # Generate sine and cosine embeddings\n",
    "    sin_embeddings = np.sin(phase_matrix)  # Shape: (M, D/2)\n",
    "    cos_embeddings = np.cos(phase_matrix)  # Shape: (M, D/2)\n",
    "\n",
    "    # Combine sine and cosine embeddings\n",
    "    # Shape: (M, D) where D = embed_dim\n",
    "    combined_embeddings = np.concatenate([sin_embeddings, cos_embeddings], axis=1)\n",
    "\n",
    "    return combined_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb98cde6-a76f-4230-b9ee-2047fa979f23",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c4ef11-e177-4fc1-87cf-cc4252dfd094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "def visualize_embeddings(embeddings, coordinate_grid, plot_freqency_components=True):\n",
    "    \"\"\"\n",
    "    Visualize how 1D sinusoidal embeddings encode positions.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: The dimensionality of the embeddings\n",
    "        coordinate_grid: Array of positions to encode, can be any shape\n",
    "    \"\"\"\n",
    "    # Create heatmap for each spatial dimension\n",
    "    n_dims = len(coordinate_grid.squeeze().shape)\n",
    "    if n_dims == 1:\n",
    "        plot_1d_heatmap(embeddings, coordinate_grid)\n",
    "    elif n_dims == 2:\n",
    "        plot_2d_heatmap(embeddings, coordinate_grid)\n",
    "    else:\n",
    "        print(f\"Heatmap visualization not supported for {n_dims}D grids\")\n",
    "        \n",
    "    # Show individual frequencies\n",
    "    if plot_freqency_components:\n",
    "        plot_frequency_components(embeddings, coordinate_grid)\n",
    "\n",
    "def visualize_1d_embeddings(embeddings, coordinate_grid):\n",
    "    \"\"\"\n",
    "    Visualize how 1D sinusoidal embeddings encode positions.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: The embeddings tensor\n",
    "        coordinate_grid: Array of positions to encode, can be any shape\n",
    "    \"\"\"\n",
    "    # Create heatmap based on input dimensionality\n",
    "    if len(embeddings.shape) == 2:  # 1D case\n",
    "        plot_1d_heatmap(embeddings, coordinate_grid)\n",
    "    elif len(embeddings.shape) == 3:  # 2D case\n",
    "        plot_2d_heatmap(embeddings, coordinate_grid)\n",
    "    \n",
    "    # Show individual frequencies\n",
    "    plot_frequency_components(embeddings, coordinate_grid)\n",
    "\n",
    "def plot_1d_heatmap(embeddings, coordinate_grid):\n",
    "    \"\"\"Plot heatmap for 1D coordinate grid.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    sns.heatmap(embeddings, cmap=\"RdBu\", center=0)\n",
    "    print(\"Embeddings:\")\n",
    "    print(embeddings.round(2))\n",
    "    plt.title(\"1D Position Embeddings\")\n",
    "    plt.xlabel(\"Embedding Dimension\")\n",
    "    plt.ylabel(\"Position\")\n",
    "    \n",
    "    # Add position values to y-axis\n",
    "    positions = coordinate_grid.flatten()\n",
    "    plt.yticks(np.arange(len(positions)) + 0.5, \n",
    "               labels=positions.round(2), \n",
    "               rotation=0)\n",
    "    plt.show()\n",
    "\n",
    "def plot_2d_heatmap(embeddings, coordinate_grid):\n",
    "    \"\"\"Plot heatmaps for 2D coordinate grid.\"\"\"\n",
    "    h, w, embed_dim = embeddings.shape\n",
    "    n_cols = min(4, embed_dim)\n",
    "    n_rows = (embed_dim + n_cols - 1) // n_cols\n",
    "    \n",
    "    plt.figure(figsize=(4*n_cols, 4*n_rows))\n",
    "    for i in range(embed_dim):\n",
    "        plt.subplot(n_rows, n_cols, i+1)\n",
    "        # Generate tick positions\n",
    "        x_ticks = np.linspace(0, w-1, min(w, 7))  # Limit to 7 ticks for readability\n",
    "        y_ticks = np.linspace(0, h-1, min(h, 7))\n",
    "        \n",
    "        sns.heatmap(embeddings[:, :, i].reshape(h, w), \n",
    "                   cmap=\"RdBu\", \n",
    "                   center=0,\n",
    "                   xticklabels=[f\"{j+1:.1f}\" for j in x_ticks],\n",
    "                   yticklabels=[f\"{j+1:.1f}\" for j in y_ticks])\n",
    "        plt.title(f\"Dimension {i}\")\n",
    "        if i % n_cols == 0:\n",
    "            plt.ylabel(\"Y Position\")\n",
    "        if i >= embed_dim - n_cols:\n",
    "            plt.xlabel(\"X Position\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_frequency_components(embeddings, coordinate_grid):\n",
    "    \"\"\"Plot individual frequency components with dimensionality handling.\"\"\"\n",
    "    # Check input dimensionality\n",
    "    if len(embeddings.shape) == 2:  # 1D case\n",
    "        n_pos, embed_dim = embeddings.shape\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        for i in range(embed_dim // 2):\n",
    "            plt.plot(coordinate_grid, \n",
    "                    embeddings[:, i], \n",
    "                    label=f\"sin_dim_{i}\", \n",
    "                    linestyle=\"-\")\n",
    "            plt.plot(coordinate_grid, \n",
    "                    embeddings[:, i + embed_dim // 2], \n",
    "                    label=f\"cos_dim_{i}\", \n",
    "                    linestyle=\"--\")\n",
    "        plt.title(\"Sinusoidal Components\")\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    else:  # 2D case\n",
    "        h, w, embed_dim = embeddings.shape\n",
    "        # Create x and y coordinate arrays\n",
    "        x_coords = np.linspace(0, w-1, w)\n",
    "        y_coords = np.linspace(0, h-1, h)\n",
    "        \n",
    "        # Create subplots for both x and y variations\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "        \n",
    "        # Plot variation along x-axis (middle row of embeddings)\n",
    "        mid_y = h // 2\n",
    "        for i in range(embed_dim // 2):\n",
    "            ax1.plot(x_coords, \n",
    "                    embeddings[mid_y, :, i], \n",
    "                    label=f\"sin_dim_{i}\", \n",
    "                    linestyle=\"-\")\n",
    "            ax1.plot(x_coords, \n",
    "                    embeddings[mid_y, :, i + embed_dim // 2], \n",
    "                    label=f\"cos_dim_{i}\", \n",
    "                    linestyle=\"--\")\n",
    "        ax1.set_title(\"Sinusoidal Components Along X-axis (at middle Y)\")\n",
    "        ax1.set_xlabel(\"X Position\")\n",
    "        ax1.set_ylabel(\"Value\")\n",
    "        ax1.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        \n",
    "        # Plot variation along y-axis (middle column of embeddings)\n",
    "        mid_x = w // 2\n",
    "        for i in range(embed_dim // 2):\n",
    "            ax2.plot(y_coords, \n",
    "                    embeddings[:, mid_x, i], \n",
    "                    label=f\"sin_dim_{i}\", \n",
    "                    linestyle=\"-\")\n",
    "            ax2.plot(y_coords, \n",
    "                    embeddings[:, mid_x, i + embed_dim // 2], \n",
    "                    label=f\"cos_dim_{i}\", \n",
    "                    linestyle=\"--\")\n",
    "        ax2.set_title(\"Sinusoidal Components Along Y-axis (at middle X)\")\n",
    "        ax2.set_xlabel(\"Y Position\")\n",
    "        ax2.set_ylabel(\"Value\")\n",
    "        ax2.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d62b6-3d84-466d-81dd-95632f807923",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_grid = np.arange(0, 2*np.pi).squeeze()\n",
    "embeddings = get_1d_sincos_pos_embed_from_grid(embed_dim=2, coordinate_grid=coordinate_grid)\n",
    "visualize_embeddings(embeddings, coordinate_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2566b-82ae-459c-b63d-844e4559208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate_grid = np.arange(0, 2*np.pi).squeeze()\n",
    "# embeddings = get_1d_sincos_pos_embed_from_grid(embed_dim=8, coordinate_grid=coordinate_grid)\n",
    "# visualize_embeddings(embeddings, coordinate_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a579a926-547c-494e-b65a-09c6b934503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coordinate_grid = np.arange(0, 2*np.pi).squeeze()\n",
    "# embeddings = get_1d_sincos_pos_embed_from_grid(embed_dim=32, coordinate_grid=coordinate_grid)\n",
    "# visualize_embeddings(embeddings, coordinate_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd19702-05c0-448b-91be-62fa30cf4e19",
   "metadata": {},
   "source": [
    "The unique encoding given to each position is given by the values for all the dimensions for that given position. Naturally, this can be extended to the 2d case as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524489f1-a7e7-4fcc-b5cb-7cb2515a147a",
   "metadata": {},
   "source": [
    "#### Implementation 2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0045b9-240b-4174-b4ec-aa1237f1b971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_2d_sincos_pos_embed_from_grid(embed_dim: int, coordinate_grid: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a 2D coordinate grid into sinusoidal positional embeddings.\n",
    "\n",
    "    Args:\n",
    "        embed_dim: Total embedding dimension (must be even)\n",
    "        coordinate_grid: Grid of coordinates with shape [2, 1, H, W]\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Positional embeddings with shape [H*W, embed_dim]\n",
    "    \"\"\"\n",
    "    assert embed_dim % 2 == 0, \"Embedding dimension must be even\"\n",
    "\n",
    "    # Split dimension evenly between height and width coordinates\n",
    "    dim_per_coordinate = embed_dim // 2\n",
    "\n",
    "    # Generate embeddings separately for height and width coordinates\n",
    "    height_embeddings = get_1d_sincos_pos_embed_from_grid(\n",
    "        dim_per_coordinate, coordinate_grid=coordinate_grid[0]\n",
    "    )  # Shape: (H*W, D/2)\n",
    "    width_embeddings = get_1d_sincos_pos_embed_from_grid(\n",
    "        dim_per_coordinate, coordinate_grid=coordinate_grid[1]\n",
    "    )  # Shape: (H*W, D/2)\n",
    "\n",
    "    # Combine height and width embeddings\n",
    "    # Shape: (H*W, D) where D = dim_per_coordinate * 2 = embed_dim\n",
    "    combined_embeddings = np.concatenate([height_embeddings, width_embeddings], axis=1)\n",
    "\n",
    "    return combined_embeddings\n",
    "\n",
    "def construct_coordinate_grid_2d(grid_size_x, grid_size_y):\n",
    "    coordinate_grid_x = np.arange(grid_size_x)\n",
    "    coordinate_grid_y = np.arange(grid_size_y)   \n",
    "    grid_h, grid_w = np.meshgrid(coordinate_grid_y, coordinate_grid_x)\n",
    "    coordinate_grid_2d = np.stack([grid_h, grid_w], axis=0)\n",
    "    coordinate_grid_2d = coordinate_grid_2d.reshape([2, 1, grid_size_y, grid_size_x])\n",
    "    return coordinate_grid_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15981e9c-372c-44f0-b884-bacda8e16150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid size determines the domain of the positional embeddings\n",
    "# Embed dim determines the number of frequencies of the positional embeddings\n",
    "# Extend embeddings to 2d, but really it is just made up of reshaped 1d embeddings so instead of (N, D) where N=number of tokens, and D=embed_dim\n",
    "# it is now (H * W, D) where H = number of tokens in H, and W = number of tokens in W)\n",
    "grid_size = int(2 * np.pi) + 1\n",
    "grid_size_x = grid_size\n",
    "grid_size_y = grid_size\n",
    "coordinate_grid_2d = construct_coordinate_grid_2d(grid_size_x=grid_size_x, grid_size_y=grid_size_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edc9632-ec3e-4a61-b6f0-0e23359d920e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets see first the H embeddings\n",
    "embed_dim = 2\n",
    "coordinate_grid_h = coordinate_grid_2d[0]\n",
    "coordinate_h, coordinate_w = coordinate_grid.squeeze().shape\n",
    "embeddings = get_1d_sincos_pos_embed_from_grid(embed_dim=embed_dim, coordinate_grid=coordinate_grid_h)\n",
    "reshaped_embeddings = embeddings.reshape(coordinate_h, coordinate_w, embed_dim)\n",
    "visualize_embeddings(reshaped_embeddings, coordinate_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a759a1a8-b942-43c8-ac48-52dfac3a5938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then the W embeddings\n",
    "embed_dim = 2\n",
    "coordinate_grid_w = coordinate_grid_2d[1]\n",
    "coordinate_h, coordinate_w = coordinate_grid.squeeze().shape\n",
    "embeddings = get_1d_sincos_pos_embed_from_grid(embed_dim=embed_dim, coordinate_grid=coordinate_grid_w)\n",
    "reshaped_embeddings = embeddings.reshape(coordinate_h, coordinate_w, embed_dim)\n",
    "visualize_embeddings(reshaped_embeddings, coordinate_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e766e1-c6ac-4f8a-86a9-26391dc42769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WLOG we can always reshape a 1d embedding into 2d thus get_2d_sincos_pos_embed_from_grid operates will reshape the embeddings from (H, W, embed_dim) -> (H * W, embed_dim)\n",
    "embed_dim_1d = 2\n",
    "embeddings = get_2d_sincos_pos_embed_from_grid(embed_dim_1d * 2, coordinate_grid_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9644f9c4-df74-4288-af34-5063a1b69c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert embeddings.shape == (grid_size_y * grid_size_x, embed_dim_1d * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c8ba87-781c-48c0-be04-f937b24e5bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotated-dust3r",
   "language": "python",
   "name": "annotated-dust3r"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
